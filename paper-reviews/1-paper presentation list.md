## Paper presentation list



We plan to present one paper for each group disccusion. Please add the presentation in the format of ``time, paper title（conference、journal), name''.

1) 9.17, iGniter: Interference-Aware GPU Resource Provisioning for Predictable DNN Inference in the Cloud（TPDS'24）, Fei Xu
2) 9.24, Optimizing Resource Allocation in Hyperscale Datacenters: Scalability, Usability, and Experiences（OSDI'24）, Zongqing Wei
3) 10.15, Metis: Fast Automatic Distributed Training on Heterogeneous GPUs （ATC’24）, Ruixing Li
4) 10.22, dLoRA: Dynamically Orchestrating Requests and Adapters for LoRA LLM Serving （OSDI'24), Lingxuan Weng
5) 10.29, StreamBox: A Lightweight GPU SandBox for Serverless Inference Workflow (ATC'24), Yikun Gu
6) 10.29, DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines (Eurosys'24), Qiannan Zhou
7) 11.6, ModelKeeper: Accelerating DNN Training via Automated Training Warmup (NSDI'23), XiangShen
8) 11.6, Starburst: A Cost-aware Scheduler for Hybrid Cloud(ATC'24), Zongqing Wei
9) 11.12, Pre-Warming is Not Enough: Accelerating Serverless Inference With Opportunistic Pre-Loading (SOCC'24), Yikun Gu
10) 11.12, mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline Parallelism in Multiple GPUs (under revision in VLDB’25), Lingxuan Weng
11) 11.19, USHER：Holistic Interference Avoidance for Resource Optimized ML Inference (OSDI'24), Xiang Shen
12) 11.19, Managing Bandwidth: The Key to Cloud-Assisted Autonomous Driving (arXiv'24), Zongqing Wei
13) 11.26, SMIless: Serving DAG-based Inference with Dynamic Invocations under Serverless Computing (SC'24), Yikun Gu
14) 11.26, Online Scheduling and Pricing for Multi-LoRA Fine-Tuning Tasks(ICPP'24), Lingxuan Weng
15) 12.3, AutoBurst: Autoscaling Burstable Instances for Cost-effective Latency SLOs(SOCC'24), Zongqing wei
16) 12.3, Optimus ：Warming Serverless ML Inference via Inter-Function Model Transformation(Eurosys'24), Xiang Shen


